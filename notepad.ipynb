{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chains\n",
    "\n",
    "A first-order Markov chain is a conditional probability distribution of the form\n",
    "$$\n",
    "    P(X_k = x_k | X_{k-1} = x_{k-1}),\n",
    "$$\n",
    "where $x_j \\in X$, $X$ is the support of $X_1,X_2,\\ldots$.\n",
    "In this context, we refer to $X$ as the set of *tokens* that may be observed.\n",
    "\n",
    "An $n$-th order Markov chain models\n",
    "$$\n",
    "    P(X_k = x_k | x_{k-1}, x_{k-2}, \\ldots, x_{k-n + 1}).\n",
    "$$\n",
    "\n",
    "We can model this with a first-order Markov chain with $|X|^n$ states.\n",
    "Some observations:\n",
    "\n",
    "1. As the token set $X$ increases in size (cardinality), the number of states\n",
    "grows by polynomially, $O(|X|^n)$.\n",
    "\n",
    "2. As the order $n$ increases, the number of states grows exponentially, but the\n",
    "   non-zero state transition probabilities stays *constant*, $O(|X|)$ and therefore transition matrix becomes increasingly sparse.\n",
    "\n",
    "Sparse matrices are pretty efficient to represent, but the number of states\n",
    "can be a problem.\n",
    "\n",
    "We consider an AR language model over bytes (256 tokens at max, although we\n",
    "only populate whatever is in the training data),\n",
    "\n",
    "$$\n",
    "    P(X_k = x_k | x_{k-1}, x_{k-2}, \\ldots, x_{k-n + 1})\n",
    "$$\n",
    "where each $x_i$ is a byte token, typically printable ASCII characters.\n",
    "\n",
    "We can thus think of this as an $n$-th order Markov chain with $O(256^n)$ states,\n",
    "also known as an $n$-gram model.\n",
    "\n",
    "> Technically, we actually store states with\n",
    "> $n-1$ tokens down to $0$ tokens, also, so the number of states is actually\n",
    "> $O(256^{n+1})$.\n",
    "\n",
    "In our model, we never enter a state, here called a *context*, that has not been\n",
    "encountered in the training data. The context is simply the last $n$ tokens observed,\n",
    "or less if we have not observed $n$ tokens yet. Since we may not use most states,\n",
    "and the transition matrix is sparse, it may make sense to represent the transition\n",
    "matrix as a dictionary of dictionaries, where the outer dictionary is indexed by\n",
    "the context and the inner dictionary is indexed by the token and contains the\n",
    "number of times that token was observed in that context in the training data.\n",
    "We can then convert this to a probability distribution by normalizing the counts\n",
    "in each context.\n",
    "\n",
    "We can then generate text by starting with a any context and then sampling from\n",
    "the probability distribution for that context to get the next token. We do something\n",
    "pretty silly in our construction -- we keep the entire history of tokens around\n",
    "to generate the next token, because we do not hard code the order of the Markov\n",
    "chain and allow it to vary for each observation or training data set.\n",
    "\n",
    "This kind of language model, the $n$-gram model, is a simple model. Compared\n",
    "to more sophisticated models, like transformer-based models, it performs quite\n",
    "poorly. Here are a few reasons why:\n",
    "\n",
    "1. The $n$-gram model is not able to capture long-range dependencies in the data\n",
    "as well, given that the number of states grows exponentially with the order\n",
    "of the model, which makes it difficult to use for large $n$.\n",
    "\n",
    "2. The $n$-gram model does not generalize well to unseen data. Since language\n",
    "is a high-dimensional space, *most* contexts are unseen in the training data.\n",
    "We represent an additional inductive bias by assuming that if a context was\n",
    "not seen previously, the next-best context is one in which the oldest token\n",
    "is dropped. This is a pretty strong assumption, and is not generally true,\n",
    "but it is a simple way to handle unseen contexts and it's often reasonable.\n",
    "\n",
    "Due to points (1) and (2) above, the $n$-gram model does not in practice capture\n",
    "the semantics of a naturla language very well. Presumably, if we had\n",
    "a sufficiently large training data set with a sufficiently large order $n$ for\n",
    "context, we could capture the semantics of the language. However, natural\n",
    "language is far too high-dimensional to represent by simply storing it; a good\n",
    "predictive model is one which **compresses** the representation\n",
    "of the data such that it can be efficiently reconstructed. This is the human\n",
    "brain operates -- we don't remember every single thing we've ever seen, but\n",
    "we remember a compressed representation of it such that we can fit some salient\n",
    "mental model of the world in our tiny little heads. This touches upon the\n",
    "hypothetical concept of *compression* as a proxy for *understanding* in the\n",
    "context of machine learning. To compress something, you must be able to\n",
    "extract the salient features and use those features to reconstruct the original\n",
    "data or predict future data.\n",
    "\n",
    "In neural linguistic models, we use a large neural network to learn a compressed\n",
    "representation of the data, and then use that representation to predict future\n",
    "data. These compressed representations naturally do things like map\n",
    "semantically similar words and phrases to similar representations, and thus\n",
    "if we prompt the model with a prompt that is semantically similar to the training\n",
    "data, it will be able to more accurately predict the next token. This is because\n",
    "the model has learned a compressed representation of the data that captures\n",
    "the semantics of the language.\n",
    "\n",
    "However, that said, here we explore this simple $n$-gram model over bytes, which\n",
    "may help us understand some aspects of how large language models work. Since\n",
    "natural language is too complex to model with a simple $n$-gram model, we train\n",
    "it on synthetic data, or algorithmic data, to see how well different orders of\n",
    "the model can capture the structure of the data and use it to generate new\n",
    "data. Indeed, we can use it by prompting it in the same way we prompt\n",
    "LLMs, and see how well it can predict the correct output, because in our data\n",
    "geenerating process (DGP), the data at some points is *deterministic*, such as\n",
    "when given the prompt `sort[1, 3, 2]`, the correct output is `[1, 2, 3]` without\n",
    "exception in the DGP. As we shorten the context, or the order, we see that the\n",
    "model becomes less and less able to predict the correct output, and the\n",
    "generated data becomes more and more random, as a distribution over the now\n",
    "shorter context. We can see that the context that was removed was salient but\n",
    "now *latent* in the data, and the model is unable to capture the full structure\n",
    "of the data.\n",
    "\n",
    "Even large language models have his problem. Even if we *assume* they had\n",
    "a sufficiently large context and model capacity, they still have two problems:\n",
    "\n",
    "(1) They do not necessarily generalize out-of-distribution as well as they\n",
    "could. This is because the data they are trained on is not necessarily\n",
    "representative of the data they will be used on\n",
    "\n",
    "(2) Because most states are out-of-distribution in these high-dimensioal\n",
    "state spaces, the model must generalize to unseen states. However, the model\n",
    "may not capture the optimal set of inductive biases for the task at hand. There\n",
    "is a sayinug in the ML community: there are no free lunches. This means that\n",
    "there is no model that is optimal for all tasks, and that all models have\n",
    "inductive biases that make them better at some tasks and worse at others. The\n",
    "trick is to find the right model for the right task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': 6, 'data': '2'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import algorithmic_data\n",
    "import importlib\n",
    "import mc2\n",
    "import math\n",
    "\n",
    "importlib.reload(mc2)\n",
    "importlib.reload(algorithmic_data)\n",
    "data = algorithmic_data.generate([\n",
    "   max, min, sorted, sum, math.prod], 10)\n",
    "\n",
    "\n",
    "algorithmic_data.generate_tree([max, min, sum, sorted, id, math.prod],\n",
    "                               .01, 5, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[110:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(algorithmic_data)\n",
    "data2 = algorithmic_data.generate_comp([sum,min,max,sorted],2000000)\n",
    "\n",
    "#print(algorithmic_data.generate_tree([sum, min, max, sorted], 3, 3, 9, '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sum[sorted[7,4],min[5,4,7]]=15.',\n",
       " 'sum[sorted[2,2,7],sum[5,1,1]]=18.',\n",
       " 'sum[sorted[7,0],max[0,4,4]]=11.',\n",
       " 'sorted[sorted[3,0],min[7,9]]=[0,3,7].']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = mc2.MarkovChain()\n",
    "mc.train(data, order = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's generate a table showing sum, sorted, prod, max, and min\n",
    "# this table will be based on the final completion\n",
    "# after the `=`, e.g\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "mc.generate('sorted[1,9,3,4]=')\n",
    "\n",
    "mc.generate('min[2,max[1,1,1,1]]=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
